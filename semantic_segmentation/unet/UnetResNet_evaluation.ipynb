{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c10e7835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import logging\n",
    "import rasterio\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from os.path import dirname as up\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "path_cur = os.path.abspath(os.getcwd())\n",
    "\n",
    "sys.path.append(path_cur)\n",
    "\n",
    "from vims_dataloader_backbone import GenDEBRIS\n",
    "\n",
    "sys.path.append(os.path.join(up(up(path_cur)), 'utils'))\n",
    "\n",
    "from vims_metrics import Evaluation, confusion_matrix\n",
    "from vims_assets import labels\n",
    "from pathlib import Path\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "root_path = up(up(path_cur))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ef078a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/rapids/notebooks/sciclone/geograd/Miranda/github/shoreline_structure/semantic_segmentation/unet'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1aec3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(options):\n",
    "    # Transformations\n",
    "    \n",
    "    transform_test = transforms.Compose([transforms.ToTensor()])\n",
    "    standardization = None # transforms.Normalize(bands_mean, bands_std)\n",
    "    \n",
    "    # Construct Data loader\n",
    "\n",
    "    dataset_test = GenDEBRIS('test', transform=transform_test, standardization = standardization, data_name = options[\"data_source\"])\n",
    "\n",
    "    test_loader = DataLoader(   dataset_test, \n",
    "                                batch_size = options['batch'], \n",
    "                                shuffle = False)\n",
    "    \n",
    "    global labels\n",
    "    # Aggregate Distribution Mixed Water, Wakes, Cloud Shadows, Waves with Marine Water\n",
    "\n",
    "#     # Use gpu or cpu\n",
    "#     if torch.cuda.is_available():\n",
    "#         device = torch.device(\"cuda\")\n",
    "#     else:\n",
    "#         device = torch.device(\"cpu\")\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    model = smp.Unet(options[\"backbone\"],\n",
    "                     encoder_weights=\"imagenet\",\n",
    "                     in_channels=options[\"input_channels\"],\n",
    "                     classes=options[\"output_channels\"],\n",
    "                    activation='softmax')\n",
    "        \n",
    "    \n",
    "#     model = UNet(input_bands = options['input_channels'], \n",
    "#                  output_classes = options['output_channels'], \n",
    "#                  hidden_channels = options['hidden_channels'])\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Load model from specific epoch to continue the training or start the evaluation\n",
    "    model_file = options['model_path']\n",
    "    logging.info('Loading model files from folder: %s' % model_file)\n",
    "\n",
    "    checkpoint = torch.load(model_file, map_location = device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    del checkpoint  # dereference\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_predicted = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (batch_idx, batch_val) in enumerate(tqdm(test_loader, desc=\"testing\")):\n",
    "\n",
    "            image = batch_val['image'].to(device)\n",
    "            target = batch_val['mask'].to(device)\n",
    "\n",
    "            logits = model(image)\n",
    "\n",
    "            # Accuracy metrics only on annotated pixels\n",
    "            logits = torch.movedim(logits, (0,1,2,3), (0,3,1,2))\n",
    "            logits = logits.reshape((-1,options['output_channels']))\n",
    "            target = target.reshape(-1)\n",
    "            mask = target != -1\n",
    "            logits = logits[mask]\n",
    "            target = target[mask]\n",
    "            \n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
    "            target = target.cpu().numpy()\n",
    "            \n",
    "            y_predicted += probs.argmax(1).tolist()\n",
    "            y_true += target.tolist()\n",
    "        \n",
    "        ####################################################################\n",
    "        # Save Scores to the .log file                                     #\n",
    "        ####################################################################\n",
    "        acc = Evaluation(y_predicted, y_true)\n",
    "        logging.info(\"\\n\")\n",
    "        logging.info(\"STATISTICS: \\n\")\n",
    "        logging.info(\"Evaluation: \" + str(acc))\n",
    "        print(\"Evaluation: \" + str(acc))\n",
    "        \"\"\"\n",
    "        Confusion matrix whose i-th row and j-th column entry indicates the number of samples with true label \n",
    "        You being i-th class and predicted label being j-th class.\n",
    "        \"\"\"\n",
    "        conf_mat = confusion_matrix(y_true, y_predicted, labels)\n",
    "        \n",
    "        logging.info(\"Confusion Matrix:  \\n\" + str(conf_mat.to_string()))\n",
    "        print(\"Confusion Matrix:  \\n\" + str(conf_mat.to_string()))\n",
    "        \n",
    "        if options['predict_masks']:\n",
    "            \n",
    "            path = os.path.join(up(up(root_path)), 'VIMS', 'NAIP', 'VA_NAIP_2018_8977', 'test')\n",
    "            ROIs = [file for file in os.listdir(path) if file.split('.')[-1]=='tif']\n",
    "\n",
    "            impute_nan = np.tile(bands_mean, (256,256,1))\n",
    "                        \n",
    "            for roi in tqdm(ROIs):\n",
    "            \n",
    "#                 roi_folder = '_'.join(['S2'] + roi.split('_')[:-1])             # Get Folder Name\n",
    "#                 roi_name = '_'.join(['S2'] + roi.split('_'))                    # Get File Name\n",
    "#                 roi_file = os.path.join(path, roi_folder,roi_name + '.tif')     # Get File path\n",
    "                \n",
    "                roi_file = os.path.join(path, roi)\n",
    "                os.makedirs(options['gen_masks_path'], exist_ok=True)\n",
    "            \n",
    "                output_image = os.path.join(options['gen_masks_path'], os.path.basename(roi_file).split('.tif')[0] + '_unet.tif')\n",
    "            \n",
    "                # Read metadata of the initial image\n",
    "                with rasterio.open(roi_file, mode ='r') as src:\n",
    "                    tags = src.tags().copy()\n",
    "                    meta = src.meta\n",
    "                    image = src.read()\n",
    "                    image = np.moveaxis(image, (0, 1, 2), (2, 0, 1))\n",
    "                    dtype = src.read(1).dtype\n",
    "            \n",
    "                # Update meta to reflect the number of layers\n",
    "                meta.update(count = 1)\n",
    "            \n",
    "                # Write it\n",
    "                with rasterio.open(output_image, 'w', **meta) as dst:\n",
    "                    \n",
    "                    # Preprocessing before prediction\n",
    "                    nan_mask = np.isnan(image)\n",
    "                    image[nan_mask] = impute_nan[nan_mask]\n",
    "            \n",
    "                    image = transform_test(image)\n",
    "                    \n",
    "                    image = standardization(image)\n",
    "                    \n",
    "                    # Image to Cuda if exist\n",
    "                    image = image.to(device)\n",
    "            \n",
    "                    # Predictions\n",
    "                    logits = model(image.unsqueeze(0))\n",
    "            \n",
    "                    probs = torch.nn.functional.softmax(logits.detach(), dim=1).cpu().numpy()\n",
    "            \n",
    "                    probs = probs.argmax(1).squeeze()+1\n",
    "                    \n",
    "                    # Write the mask with georeference\n",
    "                    dst.write_band(1, probs.astype(dtype).copy()) # In order to be in the same dtype\n",
    "                    dst.update_tags(**tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62236585",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'Image_allyear_VA_256'\n",
    "\n",
    "# # Pixel-Level class distribution for each dataset\n",
    "# \"\"\"\n",
    "# {'Image_allyear_merged_512': array([0.49019898, 0.44645175, 0.02657669, 0.03677258]), \n",
    "#  'Image_after_2010_merged_512': array([0.45526231, 0.43276168, 0.05267328, 0.05930273]), \n",
    "#  'Image_after_2010_merged_256': array([0.32490837, 0.41855632, 0.17178225, 0.08475305]), done\n",
    "#  'Image_allyear_merged_256': array([0.44056167, 0.41559786, 0.09143975, 0.05240071]), done\n",
    "#  'Image_allyear_merged_1024': array([0.50426896, 0.4565353 , 0.01033589, 0.02885985]),\n",
    "#  'Image_after_2010_merged_1024': array([0.49582348, 0.43486081, 0.02433212, 0.04498359]),\n",
    "#  'Image_after_2010_VA_512': array([0.47100772, 0.44772889, 0.01990966, 0.06135373]), \n",
    "#  'Image_after_2010_VA_256': array([0.37283283, 0.48029399, 0.04961892, 0.09725425]), done\n",
    "#  'Image_allyear_VA_512': array([0.49843776, 0.45395527, 0.01021636, 0.03739061]), \n",
    "#  'Image_allyear_VA_256': array([0.47332474, 0.44650446, 0.02387324, 0.05629757])} \n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5c28189",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = data_name\n",
    "lr = 2e-3\n",
    "backbone = 'resnet101'\n",
    "\n",
    "model_name = 'unet_{}_{}_{}'.format(backbone, lr, data_source)\n",
    "model_epoch = '100'\n",
    "\n",
    "model_path = os.path.join(up(path_cur), model_name, model_epoch, 'model.pth')\n",
    "gen_masks_path = os.path.join(up(up(root_path)), 'VIMS', 'VABP', 'paper', 'test_masks')\n",
    "\n",
    "\n",
    "options = {'batch': 32, 'backbone': backbone, 'data_source': data_source,\n",
    "          'input_channels': 3, 'output_channels': 4,\n",
    "          'model_path': model_path, 'predict_masks': False, 'gen_masks_path':gen_masks_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48e14157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load test set to memory:   8%|▊         | 19/225 [00:00<00:01, 186.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rapids/notebooks/sciclone/geograd/Miranda/github/shoreline_structure/datasets/Image_allyear_VA_256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load test set to memory: 100%|██████████| 225/225 [00:01<00:00, 182.67it/s]\n",
      "testing:   0%|          | 0/8 [00:00<?, ?it/s]/opt/conda/envs/vims/lib/python3.7/site-packages/segmentation_models_pytorch/base/modules.py:104: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.activation(x)\n",
      "testing: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: {'macroPrec': 0.6042316389104878, 'microPrec': 0.6756581412366441, 'weightPrec': 0.6778333923719474, 'macroRec': 0.6120106538418562, 'microRec': 0.6756581412366441, 'weightRec': 0.6756581412366441, 'macroF1': 0.607155289060172, 'microF1': 0.6756581412366441, 'weightF1': 0.6765190114539078, 'subsetAcc': 0.6756581412366441, 'IoU': 0.44268429093756945}\n",
      "Confusion Matrix:  \n",
      "                     Bulkhead Or Sea Wall   Rip Rap    Groin Breakwater        Sum Recall\n",
      "Bulkhead Or Sea Wall             281937.0  105064.0   1422.0     2585.0   391008.0   0.72\n",
      "Rip Rap                           93342.0  214759.0  12371.0    10133.0   330605.0   0.65\n",
      "Groin                                 4.0    6322.0  11756.0     5295.0    23377.0    0.5\n",
      "Breakwater                         6536.0   13041.0   1849.0    28930.0    50356.0   0.57\n",
      "Sum                              381819.0  339186.0  27398.0    46943.0       mPA:   0.61\n",
      "IoU                                  0.57      0.47      0.3       0.42      mIoU:   0.44\n",
      "Precision                            0.74      0.63     0.43       0.62        OA:   0.68\n",
      "F1-score                             0.73      0.64     0.46       0.59  F1-macro:   0.61\n"
     ]
    }
   ],
   "source": [
    "main(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33704346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vims",
   "language": "python",
   "name": "vims"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
